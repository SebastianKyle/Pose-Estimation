{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_lib.pose_model import SimCC, PoseSimCC\n",
    "from loc_utils.human_detection import detect_humans, draw_bboxes, crop_human_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO v5 for human detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "img_path = './test_images/people10.png'\n",
    "img, bboxes = detect_humans(yolo, img_path)\n",
    "draw_bboxes(img, bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "human_images = crop_human_regions(img, bboxes)\n",
    "\n",
    "# Display cropped human images\n",
    "for idx, human_img in enumerate(human_images):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(human_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Human {idx + 1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./model/pose_simcc_192x256.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = './model/pose_simcc_192x256.pth'\n",
    "\n",
    "deconv_layers_cfg = {\n",
    "    'num_layers': 2,\n",
    "    'num_filters': [1024, 512],\n",
    "    'kernel_sizes': [4, 4],\n",
    "    'with_bias': False\n",
    "}\n",
    "\n",
    "model = PoseSimCC(num_joints=17, input_size=(192, 256), sigma=20.0, split_ratio=4.0, deconv_layers_cfg=deconv_layers_cfg)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "print(\"Model loaded from\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_single_image_with_simcc_labels(image, keypoints, simcc_x, simcc_y, split_ratio, \n",
    "                                             visibility=None, \n",
    "                                             keypoint_names=None, \n",
    "                                             orig_size=None, \n",
    "                                             orig_img=None, \n",
    "                                             bboxes=None\n",
    "                                            ):\n",
    "    image = image.numpy().transpose(1, 2, 0)  # Convert from CHW to HWC format\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    num_keypoints = simcc_x.shape[0]\n",
    "    \n",
    "    fig, axs = plt.subplots(17 // 5 + 1, 5, figsize=(15, (17//5 + 1) * 3))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Show original image\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Original Image')\n",
    "    \n",
    "    # Process and show heatmaps\n",
    "    for i in range(num_keypoints):\n",
    "        x_dist = simcc_x[i]\n",
    "        y_dist = simcc_y[i]\n",
    "\n",
    "        heatmap = np.zeros((len(y_dist), len(x_dist)))\n",
    "        for y in range(len(y_dist)):\n",
    "            for x in range(len(x_dist)):\n",
    "                heatmap[y, x] = min(y_dist[y], x_dist[x])\n",
    "        \n",
    "        heatmap = F.interpolate(\n",
    "            torch.tensor(heatmap).unsqueeze(0).unsqueeze(0),\n",
    "            scale_factor=1.0/split_ratio,\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze().numpy()\n",
    "\n",
    "        min_val = heatmap.min()\n",
    "        max_val = heatmap.max()\n",
    "        if max_val > min_val:\n",
    "            heatmap = (heatmap - min_val) / (max_val - min_val)  # Normalize the heatmap\n",
    "        \n",
    "        heatmap = np.clip(heatmap, 0, 1)\n",
    "\n",
    "        heatmap_color = cv2.applyColorMap((heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)  # Convert to color heatmap\n",
    "        heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        heatmap_color = heatmap_color.astype(np.uint8)\n",
    "        \n",
    "        # Create a blended image with transparency\n",
    "        blended_image = cv2.addWeighted(image.copy(), 0.4, heatmap_color, 0.6, 0)\n",
    "        \n",
    "        axs[i + 1].imshow(blended_image)\n",
    "        axs[i + 1].axis('off')\n",
    "        axs[i + 1].set_title(f'Heatmap {i+1}')        \n",
    "\n",
    "    keypoints_image = cv2.resize(image.copy(), (orig_size[1], orig_size[0]))\n",
    "    if keypoint_names is not None:\n",
    "        for idx in range(keypoints.shape[0]):\n",
    "            x, y = keypoints[idx]\n",
    "            x *= orig_size[1] / image.shape[1]\n",
    "            y *= orig_size[0] / image.shape[0]\n",
    "            \n",
    "            vis = visibility[idx]\n",
    "            if x > 0 and y > 0 and vis > 0.16:\n",
    "                cv2.circle(keypoints_image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "                label = keypoint_names[idx]\n",
    "                cv2.putText(keypoints_image, label, (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "                x_min, y_min, x_max, y_max = map(int, bboxes)\n",
    "                x += x_min\n",
    "                y += y_min\n",
    "                cv2.circle(orig_img, (int(x), int(y)), 5, (0, 255, 0), -1) \n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(keypoints_image)\n",
    "        plt.axis('off')\n",
    "        plt.title('Keypoints on Image')\n",
    "        plt.show()\n",
    "    else :\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "joints_names = [\n",
    "    \"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\",\n",
    "    \"Left Shoulder\", \"Right Shoulder\", \"Left Elbow\", \"Right Elbow\",\n",
    "    \"Left Wrist\", \"Right Wrist\", \"Left Hip\", \"Right Hip\",\n",
    "    \"Left Knee\", \"Right Knee\", \"Left Ankle\", \"Right Ankle\"\n",
    "]\n",
    "\n",
    "orig_img = cv2.imread(img_path)\n",
    "\n",
    "for idx, human_img in enumerate(human_images):\n",
    "    orig_size = human_img.shape\n",
    "    input_human_img = cv2.resize(human_img, (192, 256))\n",
    "    input_human_img = input_human_img.transpose(2, 0, 1)\n",
    "    input_human_img = input_human_img / 255.0\n",
    "    input_human_img = np.expand_dims(input_human_img, axis=0)\n",
    "    input_human_img = torch.tensor(np.array(input_human_img)).float()\n",
    "\n",
    "    result = model.predict(input_human_img)\n",
    "    labels, keypoints, eval = result\n",
    "    simcc_x, simcc_y = labels\n",
    "    scores, visibility = eval\n",
    "\n",
    "    visualize_single_image_with_simcc_labels(input_human_img[0], keypoints[0], simcc_x[0], simcc_y[0], 4.0, visibility[0], joints_names, orig_size, orig_img, bboxes[idx])\n",
    "\n",
    "cv2.imwrite('./results/people10.png', orig_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
