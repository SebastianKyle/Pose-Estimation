{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_lib.pose_model import SimCC, PoseSimCC\n",
    "from dataset.dataset import SimCCDataset\n",
    "from core.loss import SimCCLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    images, keypoints, keypoint_x_labels, keypoint_y_labels, keypoint_weights = [], [], [], [], []\n",
    "    for img_set, kp_set, encoded in batch:\n",
    "        images.extend(img_set)\n",
    "        keypoints.extend(kp_set)\n",
    "        keypoint_x_labels.extend(torch.tensor(encoded['keypoints_x_labels']))\n",
    "        keypoint_y_labels.extend(torch.tensor(encoded['keypoints_y_labels']))\n",
    "        keypoint_weights.extend(torch.tensor(encoded['keypoint_weights']))\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    keypoints = torch.stack(keypoints)\n",
    "    keypoint_x_labels = torch.stack(keypoint_x_labels)\n",
    "    keypoint_y_labels = torch.stack(keypoint_y_labels)\n",
    "    keypoint_weights = torch.stack(keypoint_weights)\n",
    "\n",
    "    return images, {'keypoints': keypoints, 'keypoint_x_labels': keypoint_x_labels, 'keypoint_y_labels': keypoint_y_labels, 'keypoint_weights': keypoint_weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './train2017'\n",
    "train_annotation_file = './data/annotations/person_keypoints_train2017.json'\n",
    "val_data_dir = './val2017'\n",
    "val_annotation_file = './data/annotations/person_keypoints_val2017.json'\n",
    "\n",
    "train_dataset_cfg = {\n",
    "    'input_shape': [192, 256],\n",
    "    'num_joints': 17,\n",
    "    'augment': True,\n",
    "    'grayscale': False,\n",
    "    'split_ratio': 4.0,\n",
    "    'sigma': 20.0,\n",
    "    'weights_normalize': False,\n",
    "    'min_max_normalize': False,\n",
    "    'min_max_scale': 25.0\n",
    "}\n",
    "\n",
    "val_dataset_cfg = {\n",
    "    'input_shape': [192, 256],\n",
    "    'num_joints': 17,\n",
    "    'augment': False,\n",
    "    'grayscale': False,\n",
    "    'split_ratio': 4.0,\n",
    "    'sigma': 20.0,\n",
    "    'weights_normalize': False,\n",
    "    'min_max_normalize': False,\n",
    "    'min_max_scale': 25.0\n",
    "}\n",
    "\n",
    "train_dataset = SimCCDataset(data_dir=train_data_dir, annotation_file=train_annotation_file, dataset_cfg=train_dataset_cfg)\n",
    "val_dataset = SimCCDataset(data_dir=val_data_dir, annotation_file=val_annotation_file, dataset_cfg=val_dataset_cfg)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_single_image_with_simcc_labels(image, keypoints, simcc_x, simcc_y, split_ratio, visibility=None, keypoint_names=None):\n",
    "    image = image.numpy().transpose(1, 2, 0)  # Convert from CHW to HWC format\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    num_keypoints = simcc_x.shape[0]\n",
    "    \n",
    "    fig, axs = plt.subplots(17 // 5 + 1, 5, figsize=(15, (17//5 + 1) * 3))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Show original image\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Original Image')\n",
    "    \n",
    "    # Process and show heatmaps\n",
    "    for i in range(num_keypoints):\n",
    "        x_dist = simcc_x[i]\n",
    "        y_dist = simcc_y[i]\n",
    "\n",
    "        heatmap = np.zeros((len(y_dist), len(x_dist)))\n",
    "        for y in range(len(y_dist)):\n",
    "            for x in range(len(x_dist)):\n",
    "                heatmap[y, x] = min(y_dist[y], x_dist[x])\n",
    "        \n",
    "        heatmap = F.interpolate(\n",
    "            torch.tensor(heatmap).unsqueeze(0).unsqueeze(0),\n",
    "            scale_factor=1.0/split_ratio,\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze().numpy()\n",
    "\n",
    "        min_val = heatmap.min()\n",
    "        max_val = heatmap.max()\n",
    "        if max_val > min_val:\n",
    "            heatmap = (heatmap - min_val) / (max_val - min_val)  # Normalize the heatmap\n",
    "        \n",
    "        heatmap = np.clip(heatmap, 0, 1)\n",
    "\n",
    "        heatmap_color = cv2.applyColorMap((heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)  # Convert to color heatmap\n",
    "        heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        heatmap_color = heatmap_color.astype(np.uint8)\n",
    "        \n",
    "        # Create a blended image with transparency\n",
    "        blended_image = cv2.addWeighted(image, 0.4, heatmap_color, 0.6, 0)\n",
    "        \n",
    "        axs[i + 1].imshow(blended_image)\n",
    "        axs[i + 1].axis('off')\n",
    "        axs[i + 1].set_title(f'Heatmap {i+1}')        \n",
    "\n",
    "    if keypoint_names is not None:\n",
    "        keypoints_image = image.copy()\n",
    "        for idx in range(keypoints.shape[0]):\n",
    "            x, y = keypoints[idx]\n",
    "            vis = visibility[idx]\n",
    "            if x > 0 and y > 0 and vis > 0.15:\n",
    "                cv2.circle(keypoints_image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "                label = keypoint_names[idx]\n",
    "                cv2.putText(keypoints_image, label, (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(keypoints_image)\n",
    "        plt.axis('off')\n",
    "        plt.title('Keypoints on Image')\n",
    "        plt.show()\n",
    "    else :\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "\n",
    "keypoints = targets['keypoints']\n",
    "keypoint_x_labels = targets['keypoint_x_labels']\n",
    "keypoint_y_labels = targets['keypoint_y_labels']\n",
    "keypoint_weights = targets['keypoint_weights']\n",
    "\n",
    "visualize_single_image_with_simcc_labels(images[0], keypoints[0], keypoint_x_labels[0], keypoint_y_labels[0], 4.0, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, batch, model, optimizer, path='./model/pose_simcc.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\" - Checkpoint saved at epoch {epoch + 1}, batch {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "load_model = True\n",
    "model_path = './model/pose_simcc_192x256.pth'\n",
    "\n",
    "deconv_layers_cfg = {\n",
    "    'num_layers': 2,\n",
    "    'num_filters': [1024, 512],\n",
    "    'kernel_sizes': [4, 4],\n",
    "    'with_bias': False\n",
    "}\n",
    "\n",
    "model = PoseSimCC(num_joints=17, input_size=(192, 256), sigma=20.0, split_ratio=4.0, deconv_layers_cfg=deconv_layers_cfg).to(device)\n",
    "\n",
    "if load_model and os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    print(\"Model loaded from\", model_path)\n",
    "\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "criterion = SimCCLoss(beta=1.25, use_target_weight=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000005, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)\n",
    "\n",
    "steps_per_epoch = 1200\n",
    "validation_steps = 100\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (images, targets) in enumerate(train_loader):\n",
    "        if step >= steps_per_epoch:\n",
    "            break\n",
    "\n",
    "        keypoints = targets['keypoints']\n",
    "        keypoint_x_labels = targets['keypoint_x_labels']\n",
    "        keypoint_y_labels = targets['keypoint_y_labels']\n",
    "        keypoint_weights = targets['keypoint_weights']\n",
    "\n",
    "        images = images.to(device)\n",
    "        keypoint_x_labels = keypoint_x_labels.to(device)\n",
    "        keypoint_y_labels = keypoint_y_labels.to(device)\n",
    "        keypoint_weights = keypoint_weights.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, (keypoint_x_labels, keypoint_y_labels), keypoint_weights)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        sys.stdout.write(f'\\rEpoch [{epoch+1}/{num_epochs}], Step [{step+1}/{steps_per_epoch}], Loss: {loss.item():.4f}')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if (step + 1) % 500 == 0:  # Save model checkpoint every 500 steps\n",
    "            save_checkpoint(epoch, step + 1, model, optimizer, model_path)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for step, (images, targets) in enumerate(val_loader):\n",
    "            if step >= validation_steps:\n",
    "                break\n",
    "\n",
    "            keypoints = targets['keypoints']\n",
    "            keypoint_x_labels = targets['keypoint_x_labels']\n",
    "            keypoint_y_labels = targets['keypoint_y_labels']\n",
    "            keypoint_weights = targets['keypoint_weights']\n",
    "\n",
    "            images = images.to(device)\n",
    "            keypoint_x_labels = keypoint_x_labels.to(device)\n",
    "            keypoint_y_labels = keypoint_y_labels.to(device)\n",
    "            keypoint_weights = keypoint_weights.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, (keypoint_x_labels, keypoint_y_labels), keypoint_weights)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / steps_per_epoch\n",
    "    avg_val_loss = val_loss / validation_steps\n",
    "    print(f'\\n Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_names = [\n",
    "    'Nose', 'Left Eye', 'Right Eye', 'Left Ear', 'Right Ear',\n",
    "    'Left Shoulder', 'Right Shoulder', 'Left Elbow', 'Right Elbow',\n",
    "    'Left Wrist', 'Right Wrist', 'Left Hip', 'Right Hip',\n",
    "    'Left Knee', 'Right Knee', 'Left Ankle', 'Right Ankle'\n",
    "]\n",
    "\n",
    "images, targets = next(iter(val_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "outputs = model.predict(images)\n",
    "\n",
    "labels, keypoints, eval = outputs\n",
    "simcc_x, simcc_y = labels\n",
    "scores, visibility = eval\n",
    "\n",
    "visualize_single_image_with_simcc_labels(images[0].cpu(), keypoints[0], simcc_x[0], simcc_y[0], 4.0, visibility[0], keypoint_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to ONNX format (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you may want to load the model\n",
    "model_path = './model/pose_simcc_192x256.pth'\n",
    "\n",
    "deconv_layers_cfg = {\n",
    "    'num_layers': 2,\n",
    "    'num_filters': [1024, 512],\n",
    "    'kernel_sizes': [4, 4],\n",
    "    'with_bias': False\n",
    "}\n",
    "\n",
    "model = PoseSimCC(num_joints=17, input_size=(192, 256), sigma=20.0, split_ratio=4.0, deconv_layers_cfg=deconv_layers_cfg)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    torch.rand(1, 3, 256, 192),\n",
    "    \"./model/simcc_192x256.onnx\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
